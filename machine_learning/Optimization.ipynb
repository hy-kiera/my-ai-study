{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimization\n",
    "\n",
    "## Objective function\n",
    "The simple error function, MSE(Mean Squared Error) is $e = {1 \\over 2} \\lVert \\mathbf{y} - \\mathbf{o} \\rVert_2^2$. The gradient is a demerit which corrects weight and bias to reduce an error during machine learning. Then, if there is one output node, error can be written as  \n",
    "$$e = {1 \\over 2} (y - o)^2 = {1 \\over 2} (y - \\sigma(wx + b))^2, \\text{ which } \\sigma \\text{ is logistic sigmoid function}$$  \n",
    "And gradients are  \n",
    "$$\n",
    "\\frac {\\partial e} {\\partial w} = -(y - o)x\\sigma'(wx + b) \\\\\n",
    "\\frac {\\partial e} {\\partial b} = -(y - o)\\sigma'(wx + b)\n",
    "$$  \n",
    "Using MSE make learning slow because logistic sigmoid function. Derivative of logistic sigmoid is the biggest when input is 0, and converges to 0 when input increases or decreases. So the bigger $wx + b$, the smaller the gradient.  \n",
    "\n",
    "### Cross-Entropy function\n",
    "Cross-Entropy is  \n",
    "$$H(P, Q) = -\\textstyle \\sum_{y \\in \\{0,1\\}} P(y)\\log_2Q(y)$$  \n",
    "\n",
    "In other words,  \n",
    "$$P(0) = 1 - y \\quad Q(0) = 1 - o \\\\\n",
    "P(1) = y \\quad Q(1) = o$$  \n",
    "\n",
    "So  \n",
    "$$e = -(y\\log_2 o + (1 - y) \\log_2 (1 - o)), \\quad o = \\sigma(z), z = wx + b$$  \n",
    "\n",
    "The objective function of cross-entropy functions are  \n",
    "$$\n",
    "\\begin{alignat}{4}\n",
    "\\frac {\\partial e} {\\partial w} & = - ({y \\over o} - {1 - y \\over 1 - o})\\frac {\\partial o} {\\partial w} \\\\\n",
    "& = - ({y \\over o} - {1 - y \\over 1 - o})x\\sigma'(z) \\\\\n",
    "& = -x ({y \\over o} - {1 - y \\over 1 - o})o(1 - o) \\\\\n",
    "& = x(o - y)\n",
    "\\end{alignat}\n",
    "$$  \n",
    "Thus,  \n",
    "$$\\frac {\\partial e}{\\partial w} = x(o - y) \\\\\n",
    "\\frac {\\partial e}{\\partial b} = (o - y)$$  \n",
    "\n",
    "These are the objective function of cross entropy which has one output node. Then the objective function which has the output vector $\\mathbf{o} = (o_1, o_2, ..., o_c)^T$ is  \n",
    "$$e = - \\sum_{i=1,c} (y_i\\log_2o_i + (1 - y_i) \\log_2(1 - o_i))$$  \n",
    "\n",
    "### Log likelihood function\n",
    "For several reasons, the output node uses some different activation function than hidden nodes. One of the activation function is softmax.  \n",
    "$$o_j = {e^{s_j} \\over \\textstyle \\sum_{i=1,c} e^{s_j}}$$  \n",
    "The softmax function has the effect of activating more maximum values and suppressing smaller ones. It has a property that summation becomes 1 if you add up all the outputs.\n",
    "\n",
    "Log likelihood function uses only one node $o_y$.  \n",
    "$$e = - \\log_2 o_y$$  \n",
    "$o_y$ means the output value of the node corresponding to the sample label.  \n",
    "\n",
    "\n",
    "The softmax includes the intention to suppress non-maximal values to make them closer to zero. Therefore, the softmax function goes weel with the log likelihood objective function of seeing only the output node values of the class indicated. For these reason, deep learning often uses a combination of softmax active function and log likelihood objective function.  \n",
    "\n",
    "\n",
    "## Optimization for improvement performance\n",
    "$$\n",
    "\\text{\"}\\cdots \\text{the wisdom distilled here should be taken as a guideline, to be tried and challenged, not as as practice set in stone.}\\cdots\\text{\"} \\\\\n",
    "\\text{- } \\ulcorner \\text{Neural Networks: Tricks of the Trade} \\lrcorner\n",
    "$$  \n",
    "\n",
    "### Data preprocessing\n",
    "Data has different unit of features and some features have only positive values. Such data can be slow to converge. When multiple weights increase or decrease together, the path to the lowest point is ruffled, leading to a slower convergence rate. To avoid this problem, we normalize data. Normalization makes mean of feature value 0.  \n",
    "$$x_i^{new} = {x_i^{old} - \\mu_i \\over \\sigma_i}$$  \n",
    "Then, how about nominal value(categorical feature)? We one-hot encode the data. One-hot encoding keeps the corresponding bits hot(1) and the rest cold(0).  \n",
    "\n",
    "### Initialize weight\n",
    "We need to initailize weight **randomly** to do symmetry break which avoids symmetry weight of neural network. It does not matter whether you choose random number in Gaussian distribution or uniform distribution. However, the range of random number is important. If the weights are set to close to zero, the gradient will be very small, leading to very slow learning. On the other hand, if the weights are too big, it may goes to overfitting.  \n",
    "\n",
    "\n",
    "There are several rules of thumb for determining the range of random numbers when using a uniform distribution.  \n",
    "\n",
    "\n",
    "$$r = {1 \\over \\sqrt{n_{in}}} \\\\\n",
    "r = {\\sqrt{6} \\over \\sqrt{n_{in} + n_{out}}}$$  \n",
    "After determining r selecting one expression between aboves, generate a random number in $[-r, r]$. $n_{in}$ is the number of edges coming into the node, and $n_{out}$ is the number of edges going out to the node.  \n",
    "\n",
    "### Momentum\n",
    "Momentum smooths the current gradient using a vector $\\mathbf{v}$ representing velocity. In physics, the product of mass and velocity is momentum. For neural networks, only the velocity is used assuming the mass is 1.  \n",
    "$$\n",
    "\\begin{array}{lcl}\n",
    "\\mathbf{v} = \\alpha\\mathbf{v} - \\rho \\frac {\\partial J}{\\partial \\mathbf{\\Theta}} \\\\\n",
    "\\mathbf{\\Theta} = \\mathbf{\\Theta} + \\mathbf{v}\n",
    "\\end{array}\n",
    "$$  \n",
    "The velocity vector $\\mathbf{v}$ is an accumulation of the previous gradient. The range of $\\alpha$ is $[0,1]$, and the larger the $\\alpha$, the greater the weight on the previous gradient information and the smoother the trajectory $\\Theta$ draws. Usually, $\\alpha$ is 0.5, 0.9, or 0.99. Or starting with 0.5, the alpha value is gradually increased to reach 0.99 as the number of generations increases. Momentum reduces overshooting much more than if not applied, and consequently finds the optimal solution with much fewer iterations.  \n",
    "There is a Nesterov momentum method that improves the momentum. Nesterov momentum uses the current $\\mathbf{v}$ value to predict $\\tilde{\\mathbf{\\Theta}}$ where to go and then uses the gradient $\\frac {\\partial J}{\\partial \\mathbf{\\Theta}}\\mid_\\tilde{\\mathbf{\\Theta}}$ of the predicted location.  \n",
    "$$\n",
    "\\begin{array}{lcl}\n",
    "\\tilde{\\mathbf{\\Theta}} = \\mathbf{\\Theta} + \\alpha \\mathbf{v} \\\\\n",
    "\\mathbf{v} = \\alpha \\mathbf{v} - \\rho \\frac {\\partial J}{\\partial \\mathbf{\\Theta}}\\mid_\\tilde{\\mathbf{\\Theta}} \\\\\n",
    "\\mathbf{\\Theta} = \\mathbf{\\Theta} + \\mathbf{v}\n",
    "\\end{array}\n",
    "$$  \n",
    "\n",
    "### Adaptive learning rate\n",
    "AdaGrad(Adaptive Gradient) uses adaptive learning rate.\n",
    "$$\n",
    "\\begin{array}{lcl}\n",
    "\\mathbf{r} = \\mathbf{r} + (\\mathbf{g} \\odot \\mathbf{g}) \\\\\n",
    "\\Delta \\mathbf{\\Theta} = -{\\rho \\mathbf{g} \\over \\epsilon + \\sqrt{\\mathbf{r}}} \\\\\n",
    "\\mathbf{\\Theta} = \\mathbf{\\Theta} + \\Delta \\mathbf{\\Theta}\n",
    "\\end{array}\n",
    "$$  \n",
    "$\\mathbf{r}$ is a vector of accumulation of the previous gradient and $\\Delta \\mathbf{\\Theta}$ is an update value. The $\\epsilon$ (usually $[10^{-5}, 10^{-7}]$) prevents the denominator become zero. If $r_i$ is small, $\\left\\vert \\Delta \\theta_i \\right\\vert$ moves a little. In the contrary, if the cumulative value of the previous gradient is small, it moves a lot. Thus, ${\\rho \\over \\epsilon + \\sqrt{r_i}}$ is adaptive learning rate.  \n",
    "Looking at $\\mathbf{r} = \\mathbf{r} + (\\mathbf{g} \\odot \\mathbf{g})$, the old and new gradients play the same weight until the algorithm is finished. As a result, there is a possibility that the adaptive learning rate will approach zero when $\\mathbf{r}$ becomes larger and does not converge sufficiently.  \n",
    "\n",
    "\n",
    "RMSProp uses weighted moving average method to exponentially reduce the influence of the old gradient.  \n",
    "$$\n",
    "\\mathbf{r} = \\alpha \\mathbf{r} (1 - \\alpha)\\mathbf{g} \\odot \\mathbf{g}\n",
    "$$  \n",
    "\n",
    "\n",
    "Adam(Adaptive Moment) is an algorithm that adds momentum to RMSProp.  \n",
    "\n",
    "<img src=\"./img/3_Optimizers.gif\" width=\"30%\" height=\"30%\">  \n",
    "reference : http://cs231n.github.io/neural-networks-3/\n",
    "\n",
    "### Activation function\n",
    "Forward propagation:  \n",
    "$$\n",
    "z = \\mathbf{w}^T\\tilde{\\mathbf{x}} + b \\\\\n",
    "y = \\tau(z)\n",
    "$$  \n",
    "$\\tilde{\\mathbf{x}}$ is the signal from $l - 1^{th}$ layer to $l^{th}$ layer. If you use linear activation functions, z is linear computation so it has the same effect as one layer. Therefore we should use non-linear activation function. Step function, tanh function, ReLU function etc. are non-linear function.  \n",
    "\n",
    "\n",
    "tanh function has the range $[-1, 1]$ and it can be differentiated over the entire interval. If the value is increased to some extent, a saturation phenomenon close to 1 occurs. The parameter update happens very slowly when the derivative is close to zero.  \n",
    "\n",
    "\n",
    "ReLU(Rectified Linear Unit) function is  \n",
    "$$\n",
    "z = \\mathbf{w}^T\\tilde{\\mathbf{x}} + b \\\\\n",
    "y = ReLU(z) = \\max(0, z)\n",
    "$$  \n",
    "ReLU is linear on positive realm so that saturation phenomenon does not occur. Also, it makes neural network sparse because negative realm is zero. When the neural network becomes sparse, it is excellent for solving different change factors. There are several functions that transform ReLU: leaky ReLU, PReLU(parametic ReLU).\n",
    "\n",
    "### Batch normalization\n",
    "We use batch normalization to avoid covariate shift which is that the distribution of the sample changes during learning. One of the reason that a neural network which has deep layers cannot be learned is covariate shift. It is more efficient to normalize mini-batch trainset each than entire trainset. So,  \n",
    "\n",
    "\n",
    "$$\n",
    "\\begin{array}{lcl}\n",
    "\\mu_B = {1 \\over m} \\sum_{i=1}^m z_i  \\\\\n",
    "\\sigma_B^2 = {1 \\over m} \\sum_{i=1}^m (z_i - \\mu_B)^2 \\\\\n",
    "\\tilde{z}_i = {z_i - \\mu_B \\over \\sqrt{\\sigma_B^2 + \\epsilon}}, \\quad i = 1, 2, ..., m \\\\\n",
    "z'_i = \\gamma \\tilde{z}_i + \\beta, \\quad i = 1, 2, ..., m\n",
    "\\end{array}\n",
    "$$  \n",
    "\n",
    "$\\gamma$ and $\\beta$ are hyper parameters.  \n",
    "Batch normalization has two positive effects:  \n",
    " - The initial value of the parameter is less sensitive.\n",
    " - By setting the learning rate large, the convergence speed can be improved.  \n",
    " \n",
    " \n",
    "## Regularization\n",
    "### Weight penalty  \n",
    "$$J_{regularized}(\\Theta;\\mathbb{X}, \\mathbb{Y}) = J(\\Theta;\\mathbb{X}, \\mathbb{Y}) + \\lambda R(\\Theta)$$  \n",
    "$J$ is an objective function, and $J_{regularized}$ is a regularized one. $R(\\Theta)$ is a penalty for weight. $R$ is the original prior knowledge regardless of the dataset. $\\Theta$, the parameter includes weights and bias. However, we donot regularize bias because bias does not need to be regulated in relation to only one node. If you regularize bias, then bias can be underfitting.  \n",
    " - **L2 Norm(weight decay)**  \n",
    " The penalty is squared of L2 norm.  \n",
    " $$J_{regularized}(\\Theta;\\mathbb{X}, \\mathbb{Y}) = J(\\Theta;\\mathbb{X}, \\mathbb{Y}) + \\lambda \\lVert \\Theta \\rVert_2^2$$  \n",
    " The gradient is  \n",
    " $$\\nabla J_{regularized}(\\Theta;\\mathbb{X}, \\mathbb{Y}) = \\nabla J(\\Theta;\\mathbb{X}, \\mathbb{Y}) + 2 \\lambda \\Theta\n",
    " $$  \n",
    " So, update parameter using gradient.  \n",
    " $$\n",
    " \\begin{alignat}{3}\n",
    " \\Theta & = \\Theta - \\rho\\nabla J_{regularized}(\\Theta;\\mathbb{X}, \\mathbb{Y}) \\\\\n",
    " & = \\Theta - \\rho(\\nabla J(\\Theta;\\mathbb{X}, \\mathbb{Y}) + 2\\lambda \\Theta) \\\\\n",
    " & = (1 - 2 \\rho \\lambda)\\Theta - \\rho \\nabla J(\\Theta;\\mathbb{X}, \\mathbb{Y})\n",
    " \\end{alignat}\n",
    " $$  \n",
    " $\\rho$ is a learning rate, and $\\lambda$ is a coefficient of L2 norm. Learning rate is usually set a number much smaller than 1 so $2\\rho\\lambda$ is a number samller than 1. Thus, it reduces the parameter by $2\\rho\\lambda$ and then adds $-\\rho\\nabla J$.  \n",
    " \n",
    " \n",
    " - **L1 Norm**  \n",
    " It is the sum of the absolute values of the parameter values.  \n",
    " $$J_{regularized}(\\Theta;\\mathbb{X}, \\mathbb{Y}) = J(\\Theta;\\mathbb{X}, \\mathbb{Y}) + \\lambda \\left\\vert \\Theta \\right\\vert_1$$  \n",
    " The gradient is  \n",
    " $$\\nabla J_{regularized}(\\Theta;\\mathbb{X}, \\mathbb{Y}) = \\nabla J(\\Theta;\\mathbb{X}, \\mathbb{Y}) + \\lambda\\mathbf{sign}(\\Theta)\n",
    " $$  \n",
    " $\\mathbf{sign}(\\Theta)$ is a vector of sign for each parameter. If an element of the vector is positive, then it has 1. Otherwise, -1.  \n",
    " So, update parameter using gradient.  \n",
    " $$\n",
    " \\begin{alignat}{3}\n",
    " \\Theta & = \\Theta - \\rho\\nabla J_{regularized}(\\Theta;\\mathbb{X}, \\mathbb{Y}) \\\\\n",
    " & = \\Theta - \\rho(\\nabla J(\\Theta;\\mathbb{X}, \\mathbb{Y}) + \\lambda\\mathbf{sign}(\\Theta)) \\\\\n",
    " & = \\Theta - \\rho \\nabla J(\\Theta;\\mathbb{X}, \\mathbb{Y}) - \\rho \\lambda\\mathbf{sign}(\\Theta)\n",
    " \\end{alignat}\n",
    " $$  \n",
    " $\\Theta$ moves as much as $-\\rho\\nabla J$ and additionaly moves by $\\mathbf{sign}(\\Theta)$. If you use L1 norm, saprsity can occur, which is a phenomenon that large number of parameters become zero.  \n",
    "\n",
    "### Early stopping\n",
    "The longer model learn, the more optimal model reaches. But beyond some point, the model start to memorize the training data, and the performance of the validation set is getting worse. In other words, the generalization ability begins to fall. Therefore, the strategy of stopping learning at the point of generalization ability is the most effective.  \n",
    "\n",
    "### Data augmentation\n",
    "The most obvious way to prevent overfitting is to use a sufficiently large training set. One practical way to increase the amount of data at a lower cost is to artificially modify the data you have. Such as affine transformation. Note that the degree of transformation is so large that it can be changed to another class.  \n",
    "\n",
    "### Dropout\n",
    "Dropout is an operation of randomly selecting and removing some nodes(with edges) of an input layer and a hidden layer at a predetermined ratio. Dropout can be seen as a kind of ensemble. However it is very difficult to set up appropriate hyperparameters for each dropped out neural network and train them. We can solve this problem by weight sharing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

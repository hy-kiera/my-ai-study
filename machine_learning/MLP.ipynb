{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<script type=\"text/x-mathjax-config\">\n",
    "MathJax.Hub.Config({\n",
    "    displayAlign: \"center\"\n",
    "});\n",
    "</script>\n",
    "\n",
    "# MLP(Multi-Layer Perceptron)\n",
    "\n",
    "## Perceptron\n",
    "$$\\begin{cases}\n",
    "y = τ(s) \\\\\n",
    "s = w_0 + \\sum_{i=1}^d w_i x_i, & τ(s) = \n",
    "\\begin{cases}\n",
    "1 & s \\ge 0 \\\\\n",
    "-1 & s < 0\n",
    "\\end{cases}\n",
    "\\end{cases}$$\n",
    "\n",
    "## Object function and Delta Rule\n",
    "$w = (w_0, w_1, w_2, ... , w_d)^T$  \n",
    "Object function : $J(w)$  \n",
    " * $J(w) \\ge 0$\n",
    " * if w is optimized, $J(w) = 0$\n",
    " * more error in w, bigger $J(w)$\n",
    "\n",
    "$$J(w) = \\sum_{x_k \\in Y} -y_k(w^T x_k)$$  \n",
    "\n",
    "Partially differentiate $J(w)$ by $w_i$ to compute the gradient $g$  \n",
    "\n",
    "\n",
    "$$\\frac{\\partial J(w)}{\\partial w_i} = \\sum_{x_k \\in Y$} -y_k x_{ki}, \\quad i = 0, 1, ... d$$\n",
    "\n",
    "Delta Rule  \n",
    "$$w_i = w_i + ρ \\sum_{x_k \\in Y} y_k x_{ki}, \\quad i = 0, 1, ... d$$\n",
    "\n",
    "## Activation Function\n",
    "|Function name|Function|First derivative|Range|\n",
    "|:-------:|:---------------------:|:-----------------------------------:|:---:|\n",
    "|Step|$$τ(s) = \\begin{cases} 1 & s \\ge 0 \\\\ -1 & s < 0 \\end{cases}$$|$$τ'(s) = \\begin{cases} 0 & s \\ne 0 \\\\ \\text{none} & s = 0 \\end{cases}$$|-1 and 1|\n",
    "|Logistic Sigmoid|$$τ(s) = {1 \\over 1 + e^{-as}}$$|$$τ'(s) = aτ(s)(1 - τ(s))$$|(0, 1)|\n",
    "|Hyperbolic Tanh|$$τ(s) = {2 \\over 1 + e^{-as}} - 1$$|$$τ'(s) = {a \\over 2}(1 - τ(s)^2)$$|(-1, 1)|\n",
    "|Softplus|$$τ(s) = log_e(1 + e^s)$$|$$τ'(s) = {1 \\over 1 + e^{-s}}$$|(0, ∞)|\n",
    "|ReLU|$$τ(s) = max(0, s)$$|$$τ'(s) = \\begin{cases} 0 & s < 0 \\\\ 1 & s > 0 \\\\ \\text{none} & s = 0 \\end{cases}$$|[0, ∞)|\n",
    "\n",
    "\n",
    "## Multi-Layer Perceptron\n",
    "Assume two layer perceptron which is input layer-hidden layer-output layer  \n",
    "<img src=\"./img/1_MLP.png\" width=\"40%\" height=\"40%\">\n",
    "\n",
    "$j^{th}$ hidden node computation:  \n",
    "$$z_j = τ(zsum_j), \\quad j = 1, 2, ..., p$$\n",
    "$$zsum_j = \\mathbf{u}_j^1 \\mathbf{x}$$    \n",
    "$k^{th}$ output node computation:  \n",
    "$$o_k = τ(osum_k), \\quad k = 1, 2, ..., c$$  \n",
    "$$osum_k = \\mathbf{u}_k^2 \\mathbf{z}$$  \n",
    "\n",
    "And they can be represented by matrix:  \n",
    "$$\\mathbf{o} = \\mathbf{τ}(\\mathbf{U}^2\\mathbf{τ}_h(\\mathbf{U}^1\\mathbf{x}))$$\n",
    "\n",
    "\n",
    "\n",
    "Object function $J(w)$ and given Data ${\\mathbb{X}, \\mathbb{Y}}$  \n",
    "$\\mathbb{X} = \\{\\mathbf{x}_1, \\mathbf{x}_2, ..., \\mathbf{x}_n\\}, \\mathbb{Y} = \\{\\mathbf{y}_1, \\mathbf{y}_2, ..., \\mathbf{y}_n\\}$  \n",
    "They can be wrote by a feature vector matrix $\\mathbf{X}(n*d)$, and a label matrix $\\mathbf{Y}(n*c)$  \n",
    "\n",
    "\n",
    "$$\n",
    "\\mathbf{X} = \\begin{pmatrix}\n",
    "{\\mathbf{x}_1}^T \\\\\n",
    "{\\mathbf{x}_2}^T \\\\\n",
    "\\vdots \\\\\n",
    "{\\mathbf{x}_n}^T\n",
    "\\end{pmatrix},\n",
    "\\mathbf{Y} = \\begin{pmatrix}\n",
    "{\\mathbf{y}_1}^T \\\\\n",
    "{\\mathbf{y}_2}^T \\\\\n",
    "\\vdots \\\\\n",
    "{\\mathbf{y}_n}^T\n",
    "\\end{pmatrix}\n",
    "$$\n",
    "\n",
    "The object is to find the optimized function $\\mathbf{f}$ which is mapping $\\mathbf{X}$ to $\\mathbf{Y}$ perfectly. In other words, it is to find the classifier $\\mathbf{f}$ classifing all samples correctly.  \n",
    "\n",
    "\n",
    "$$\\mathbf{Y} = \\mathbf{f}(\\mathbf{X})$$\n",
    "\n",
    "So, the machine learning do compute ($Θ = {\\mathbf{U}^1, \\mathbf{U}^2}$)  \n",
    "$$\\widehat{Θ} = \\underset{Θ}{\\text{argmin}} \\parallel\\mathbf{f}(\\mathbf{X};Θ) - \\mathbf{Y}\\parallel_2^2$$  \n",
    "\n",
    "The object function is\n",
    "$$J(Θ) = {1 \\over 2}\\parallel\\mathbf{y} - \\mathbf{o}(Θ)\\parallel_2^2$$  \n",
    "\n",
    "### Backpropagation\n",
    "$$\n",
    "\\mathbf{U}^1 = \\mathbf{U}^1 - ρ \\frac{\\partial J}{\\partial \\mathbf{U}^1} \\\\\n",
    "\\mathbf{U}^2 = \\mathbf{U}^2 - ρ \\frac{\\partial J}{\\partial \\mathbf{U}^2}\n",
    "$$\n",
    "\n",
    "\n",
    "Compute $\\frac{\\partial J}{\\partial \\mathbf{U}^1}$ and $\\frac{\\partial J}{\\partial \\mathbf{U}^2}$ to get gradient by backpropagation  \n",
    "\n",
    "\n",
    "First, compute $\\frac{\\partial J}{\\partial u_{kj}^2}$ because $\\mathbf{U}^2 is directly connected with output layer  \n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\frac{\\partial J}{\\partial u_{kj}^2}\n",
    "& = \\frac{\\partial (0.5 \\parallel\\mathbf{y} - \\mathbf{o}(\\mathbf{U}^1, \\mathbf{U}^2))\\parallel_2^2)}{\\partial u_{kj}^2} \\\\\n",
    "& = \\frac{\\partial (0.5(y_k - o_k)^2)}{\\partial u_{kj}^2} \\\\\n",
    "& = -(y_k - o_k)\\frac{\\partial o_k}{\\partial u_{kj}^2} \\\\\n",
    "& = -(y_k - o_k)\\frac{\\partial τ(osum_k)}{\\partial u_{kj}^2} \\\\\n",
    "& = -(y_k - o_k)τ'(osum_k)\\frac{\\partial osum_k}{\\partial u_{kj}^2} \\\\\n",
    "& = -(y_k - o_k)τ'(osum_k)z_j\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "Thus,   \n",
    "$$\n",
    "δ_k = (y_k - o_k)τ'(osum_k), \\quad 1 \\le k \\le c$$\n",
    "$$\\frac{\\partial J}{\\partial u_{kj}^2} = Δu_{kj}^2 = -δ_k z_j, \\quad 0 \\le j \\le p, 1 \\le k \\le c\n",
    "$$  \n",
    "\n",
    "\n",
    "\n",
    "Compute $\\frac{\\partial J}{\\partial u_{ji}^1}$. $u_{ji}^1$ affects more nodes than $u_{kj}^2$  \n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\frac{\\partial J}{\\partial u_{ji}^1}\n",
    "& = \\frac{\\partial (0.5 \\parallel\\mathbf{y} - \\mathbf{o}(\\mathbf{U}^1, \\mathbf{U}^2))\\parallel_2^2)}{\\partial u_{ji}^1} \\\\\n",
    "& = - \\sum_{q=1}^c (y_q - o_q) \\frac{\\partial o_q}{\\partial u_{ji}^1} \\\\\n",
    "& = - \\sum_{q=1}^c (y_q - o_q)τ'(osum_q) \\frac{\\partial osum_q}{\\partial u_{ji}^1} \\\\\n",
    "& = - \\sum_{q=1}^c (y_q - o_q)τ'(osum_q) \\frac{\\partial osum_q}{\\partial z_j} \\frac{\\partial z_j}{\\partial u_{ji}^1} \\\\\n",
    "& = - \\sum_{q=1}^c (y_q - o_q)τ'(osum_q)u_{qj}^2 \\frac{\\partial z_j}{\\partial u_{ji}^1} \\\\\n",
    "& = -τ'(zsum_j)x_i \\sum_{q=1}^c (y_q - o_q)τ'(osum_q)u_{qj}^2\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "Thus,   \n",
    "$$\n",
    "η_j = τ'(zsum_j) \\sum_{q=1}^c δ_q u_{qj}^2 \\quad 1 \\le j \\le p$$\n",
    "$$\\frac{\\partial J}{\\partial u_{ji}^1} = Δu_{ji}^1 = -η_j x_i, \\quad 0 \\le i \\le d, 1 \\le j \\le p\n",
    "$$"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reinforcement Learning\n",
    "\n",
    "## Principles and Properties of RL\n",
    "\n",
    "Reinforcement learning achieves goals by alternating state changes and actions, and giving reward. It does sequentially over time.  \n",
    "\n",
    "\n",
    "$$\n",
    "f:(s_t, a_t) \\rightarrow (s_{t+1}, r_{t+1})\n",
    "$$  \n",
    "By this operation, taking action $a_t$ in state $s_t$ changes to the new state $s_{t+1}$ and receives the reward $r_{t+1}$. It is, however, very difficult to set the proper reward every moment. Although the last reward is \n",
    "obvious, the reward of the last moment must be divided into the previous state, since the state of the last moment was established under the influence of the intermediate state. It is called credit assignment problem.  \n",
    "\n",
    "\n",
    "<img src=\"./img/6_RL_model.png\" width=\"40%\" height=\"40%\">  \n",
    "\n",
    "In order for the above equation to work, we need a something to determine the behavior, switch the state, and determine the amount of reward. The agent does former, and the environment does rest. MDP(Markov decision process) is the information when defining a problem. The environment decides the next state and reward by MDP. The agent decides an action by the policy as a rule for optimization.  \n",
    "The goal of reinforcement learning is that maximize of the 'cumulative' amount of reward. However, reinforcement learning should proceed in a way that maximizes the cumulative compensation amount by receiving more satisfaction than damage at every moment. So sometimes it has to take risks.  \n",
    "\n",
    "### Exploration and Exploitation\n",
    "There is **exploration and exploitation conflict** in optimization problem. Exploration is a strategy of searching the entire space evenly, and exploration is a strategy of intensively searching around a specific place. The former has a problem of taking too long time, and the latter has a problem of missing the better solution elsewhere and staying at the local optimum. In reinforcement learning, it is very likely that exploitation will lead to inferior harm. Therefore, the Monte Carlo method of sampling should be used, or the exploration and exploration should be balanced in temporal learning.  \n",
    "\n",
    "### Markov decision process\n",
    "The most important operation in reinforcement learning is to decide the most benefitial action under any status. The previous history is not important when deciding what to do in the current state. If a task satisfies this condition, it is said to have Markov property.  \n",
    "\n",
    "\n",
    "$$\n",
    "\\Pr(s_{t+1}, r_{t+1} \\vert s_0, a_0, r_1, s_1, a_1, \\cdots, r_{t-1}, s_{t-1}, a_{t-1}, r_t, s_t, a_t) = \\Pr(s_{t+1}, r_{t+1} \\vert s_t, a_t)\n",
    "$$\n",
    "a.k.a\n",
    "$$\n",
    "\\Pr(s_{t+1}, r_{t+1} \\vert s_t, a_t) = \\Pr(s', r \\vert s, a)\n",
    "$$  \n",
    "\n",
    "It means that the information of $t$ at the present moment can be determined without damaging the information at the next moment.  \n",
    "Unfortunately, not all application problems satisfy Markov property. Reinforcement learning works on the premise that it satisfies the Markov property. Thus the state expression should be designed to satisfy the problem that deviates greatly from the Markov property.  \n",
    "\n",
    "\n",
    "The agent selects an action in the current state. When the agent takes the action chosen, the environment must change to a new state and reward it. In the process, however, all the stats, all actions, and all rewards can occur so the environment should be defined by  \n",
    "\n",
    "\n",
    "$$\n",
    "\\text{MDP probability distribution: } \\Pr(s', r \\vert s, a), \\forall s \\in \\mathcal{S}, \\forall a \\in \\mathcal{A}, \\forall s' \\in \\mathcal{S}, \\forall r \\in \\mathcal{R}\n",
    "$$\n",
    "$\\mathcal{S} = \\{\\mathcal{s}_1, \\mathcal{s}_2, \\cdots, \\mathcal{s}_n\\}$ is the set of all possible state values, $\\mathcal{s}$. $\\mathcal{A} = \\{\\mathcal{a}_1, \\mathcal{a}_2, \\cdots, \\mathcal{a}_m\\}$ is the set of action values, $\\mathcal{a}$. And $\\mathcal{R} = \\{\\mathcal{r}_1, \\mathcal{r}_2, \\cdots, \\mathcal{r}_l\\}$ is the set of reward values, $\\mathcal{r}$.  \n",
    "\n",
    "\n",
    "## Policy and Value function\n",
    "The core of reinforcement learning is to find a good policy. A policy($\\pi$) is a statement of the probability of taking action $a$ in state $s$ for every state and every action.  \n",
    "\n",
    "\n",
    "$$\n",
    "\\pi(a \\vert s) = \\Pr (a \\vert s), \\forall s, \\forall a\n",
    "$$  \n",
    "\n",
    "\n",
    "A good policy is to reach the target state in the shortest time and receive the highest cumulative reward. In other words, the task of the learning algorithm is to find the optimal policy, $\\hat\\pi$.  \n",
    "\n",
    "\n",
    "$$\n",
    "\\hat\\pi = \\underset{\\pi}{\\text{argmax }} goodness(\\pi)\n",
    "$$\n",
    "$goodness(\\pi)$ is a function to measure quality of a policy, $\\pi$.  \n",
    "You can find $\\hat\\pi$ by value function. Usually state space is huge, and policy space, which is a set of different policies, is bigger than state space. So it is impossible to find the best policy using exhaustive search.  \n",
    "Value functions evaluate the goodness of a particular policy. Specifically, it evaluates the goodness of all states in a particular policy. A goodness is an estimate of cumulative reward from state $s$ to the end state. The value function is estimated by a particular policy and is denoted by $v_\\pi(s)$ as a function of state $s$.  \n",
    "\n",
    "\n",
    "$$\n",
    "\\hat\\pi = \\underset{\\pi}{\\text{argmax }} v_\\pi(s), \\forall s \\in \\mathcal{S} \\\\\n",
    "v_\\pi(s) = \\sum_{\\forall z} \\Pr(z)\\mathbb{r}(z)\n",
    "$$\n",
    "$z$ is a route from $s$ to the end state, $\\Pr(z)$ is a probability of occurence of a route $z$, and $\\mathbb{r}(z)$ is a cumulative reward of a route $z$, $\\mathbb{r}(z) = r_{t+1} + r_{t+2} + \\cdots + r_T$.  \n",
    "The route $z$ can be expressed as follows, so the blue part is $z$.  \n",
    "- finite route(episode task) $z:(s_t, r_t) \\overset{a_t}{\\rightarrow} {\\color{Blue}{(s_{t+1}, r_{t+1}) \\overset{a_{t+1}}{\\rightarrow} (s_{t+2}, r_{t+2}) \\overset{a_{t+2}}{\\rightarrow} \\cdots \\overset{a_{T-1}}{\\rightarrow} (s_T, r_T)}}$  \n",
    "- infinite route(continuing task) $z:(s_t, r_t) \\overset{a_t}{\\rightarrow} {\\color{Blue}{(s_{t+1}, r_{t+1}) \\overset{a_{t+1}}{\\rightarrow} (s_{t+2}, r_{t+2}) \\overset{a_{t+2}}{\\rightarrow} (s_{t+3}, r_{t+3}) \\overset{a_{t+3}}{\\rightarrow} (s_{t+4}, r_{t+4}) \\cdots}}$  \n",
    "\n",
    "\n",
    "In continuing task, the cumulative reward can be infinity so it uses discounted accumulating reward.  \n",
    "\n",
    "\n",
    "$$\n",
    "\\mathbb{r}(z) = r_{t+1} + \\gamma r_{t+2} + \\gamma^2 r_{t+3} + \\gamma^3 r_{t+4} + \\cdots = \\sum_{k=1}^\\infty \\gamma^{k-1} r_{t+k}\n",
    "$$\n",
    "$\\gamma$ is a discount rate, and $0 \\le \\gamma \\le 1$. If $\\gamma$ is 0, it is a very shortsighted compensation, since only $r_{t+1}$ remains, or if it is 1, then it is same with above. The discounted accumulating reward is the farther from the present moment, the lower the contribution by discounting the reward.  \n",
    "\n",
    "\n",
    "The value function has recurrent struct. So you can rewrite it like this:  \n",
    "\n",
    "\n",
    "$$\n",
    "v_\\pi(s) = \\sum_{a \\in \\mathcal{A}(s)} \\Pr(a \\vert s)(r + v_\\pi(s')), \\forall s \\in \\mathcal{S}\n",
    "$$\n",
    "$r$ is a reward when acting an action $a$ in a state $s$, and $s'$ is a next state after an action $a$ in a state $s$. Both $r$ and $s$ are function of $s$ and $a$. The value function should estimate values of all possible states.  \n",
    "\n",
    "\n",
    "**Stochastic process**  \n",
    "So far all have been deterministic processes. In other words, if you take a certain action in a state, you switch to a predetermined state and receive a certain reward. If you later choose the same behavior in the same state, you switch to the same state and get the same reward. However, it doesn't explain many applications. For example, there are many factors that govern the environment. In reality, many factors cannot be reflected in state variables, so only the main factors are reflected and the others are ignored. In this situation, when you take a certain action in a state, you may switch from one state to another and the rewards may vary. It is called by stochastic process.  \n",
    "\n",
    "\n",
    "$$\n",
    "v_\\pi(s) = \\sum_{a \\in \\mathcal{A}(s)} \\Pr(a \\vert s) \\sum_{s'} \\sum_{r} \\Pr(s', r \\vert s, a)\\left(r + v_\\pi(s')\\right), \\forall s \\in \\mathcal{S}\n",
    "$$\n",
    "$\\Pr(s', r \\vert s, a)$ is the probability of switching to state $s'$ and receiving reward $r$ when taking action $a$ in state $s$, and it is MDP probability distribution. The value function evaluates the policy $\\pi$ using the information given by $\\Pr(s', r \\vert s, a)$ and policy $\\pi$.  \n",
    "If the problem includes infinite task, apply a discount rate.  \n",
    "\n",
    "\n",
    "$$\n",
    "v_\\pi(s) = \\sum_{a \\in \\mathcal{A}(s)} \\Pr(a \\vert s) \\sum_{s'} \\sum_{r} \\Pr(s', r \\vert s, a)\\left(r + \\gamma v_\\pi(s')\\right), \\forall s \\in \\mathcal{S}\n",
    "$$  \n",
    "Both equations simply represents a relationship between value of current state and next state, and they are called by Bellman equation for value functions.  \n",
    "\n",
    "\n",
    "Unlike the above equation, the following is the value function given the state and action, state-action value function. The relationship of $v_\\pi(s) = \\textstyle \\sum_{a \\in \\mathcal{A}(s)} \\Pr(a \\vert s)q_\\pi(s, a)$ holds in this function.  \n",
    "\n",
    "\n",
    "$$\n",
    "q_\\pi(s, a) = \\sum_{s'} \\sum_{r} \\Pr(s', r \\vert s, a)\\left(r + \\gamma v_\\pi(s')\\right), \\forall s \\in \\mathcal{S}, \\forall a \\in \\mathcal{A}\n",
    "$$  \n",
    "\n",
    "\n",
    "### Optimal value function\n",
    "The optimal value function is  \n",
    "\n",
    "\n",
    "$$\n",
    "\\hat{v}(s) = v_{\\hat\\pi}(s) = \\underset{\\pi}{\\text{max }} v_\\pi(s), \\forall s \\in \\mathcal{S}\n",
    "$$  \n",
    "And the following is a method estimating the optimal value function.  \n",
    "\n",
    "\n",
    "$$\n",
    "\\hat{v}(s) = \\underset{a \\in \\mathcal{A}(s)}{\\text{max }} \\sum_{s'} \\sum_{r} \\Pr(s', r \\vert s, a)\\left(r + \\hat v(s')\\right), \\forall s \\in \\mathcal{S}\n",
    "$$  \n",
    "The process of calculating the value function above averages the cumulative reward of all actions that can be taken in state $s$, ie, applies the $mean$ operator to find the value.  \n",
    "\n",
    "\n",
    "$$\n",
    "\\hat{v}(s) = \\underset{a \\in \\mathcal{A}(s)}{\\text{max }} \\sum_{s'} \\sum_{r} \\Pr(s', r \\vert s, a)\\left(r + \\gamma \\hat v(s')\\right), \\forall s \\in \\mathcal{S}\n",
    "$$\n",
    "It is applied with a discount rate for infinite route.  \n",
    "These apply to all states, so if there are $n$ states, there are $n$ expressions. Reinforcement learning is set to a random value initially and the value function is calculated according to the policy to improve the policy. This process is repeated until there is no further policy improvement.  \n",
    "<img src=\"./img/6_find_optimal_policy.png\" width=\"40%\" height=\"40%\">  \n",
    "\n",
    "\n",
    "## Dynamic programming\n",
    "Dynamic programming must be given the MDP probability distribution, and the condition that computation time and amount of memory are realistic. This can be applied to really small problems compared to the scale of modern machine learning. However, this provides key concepts and ideas that underlie modern algorithms such as Monte Carlo methods and temporal difference learning.  \n",
    "Dynamic programming is a bottom-up method, requiring a recursive expression of the $(i+1)^{th}$ answer as the $i^{th}$ answer. It solve usually deterministic problems so it cannot be applied to reinforcement learning as it is. Thus, dynamic programming used in reinforcement learning is stochastic dynamic programming.  \n",
    "\n",
    "\n",
    "**Policy iteration algorithm**  \n",
    "The policy iteration algorithm repeats the evaluation and improvement cycles until the optimal policy is found. Since the evaluation step should be computed for all states, it takes long time. Moreover, the policy iteration algorithm takes a lot of time because it has to iterate until the evaluation-improvement cycle converges.  \n",
    "\n",
    "\n",
    "**Value iteration algorithm**  \n",
    "The value iteration algorithm finds the optimal value function by looking at the input MDP and then finds the optimal policy from the optimal value function. It needs the equation updating the present value function better.  \n",
    "\n",
    "\n",
    "$$\n",
    "v^{(t+!)}(s) = \\underset{a \\in \\mathcal{A}(s)}{\\text{max }} \\sum_{s'} \\sum_{r} \\Pr(s', r \\vert s, a) \\left(r + v^{(t)}(s')\\right), \\forall s \\in \\mathcal{S}\n",
    "$$\n",
    "$v^{(t)}(s)$ is the value function after reapting $t$ times. At first glance, it might be greedy because it is acting at $\\text{max}$ in the current state, and it is maximizing the instantaneous gain, and not the long-term gain. However the value function calculated so far implies long-term gains. After getting the optimal value function, we allocate the probability to action $a$ in $\\mathcal{A}(s)$ so that $\\textstyle \\sum_{a \\in \\mathcal{A}(s)} \\Pr(a \\vert s) = 1$, and then store the probability in $\\hat\\pi$. $\\mathcal{A}(s)$ is a set of the optimal actions in $s$. If elements of $\\mathcal{A}(s)$ is multiple, whatever action is taken to ensure optimality.  \n",
    "\n",
    "\n",
    "Both algorithms update their values by looking at values in different states. This method is called bootstrap.  \n",
    "\n",
    "\n",
    "## Monte Carlo method\n",
    "If you do not have complete information of the model but can generate samples from incomplete model information or collect samples directly through agent-environment interactions, you can use the sample as a training set to find the optimal policy. The Monte Carlo method is based on 'learning' using training sets.  \n",
    "Mark the episodes collected using the appropriate method as follows:  \n",
    "\n",
    "\n",
    "$$\n",
    "e = [s_0, r_0]a_0[s_1,r_1]a_1[s_2,r_2]a_2 \\cdots [s_t,r_t]a_t \\cdots [s_T,r_T]\n",
    "$$\n",
    "You can collect several samples from this episode. There are two collecting method, first visit and every visit. Though first visit accepts only the first occurrence if the same status occurs more than once, every visit accepts everything.  \n",
    "For example, Assume an episode occurs like this,  \n",
    "\n",
    "\n",
    "$$\n",
    "e = [10,0]E[11,0]N[7,0]S[11,0]W[10,0]W[9,0]N[5,0]N[1,5]\n",
    "$$  \n",
    "Then the state $10$ occurs at $t=0$ and $t=4$. In first visit, it allows only $t=0$, so it collect a sample $[10,0]E[11,0]N[7,0]S[11,0]W[10,0]W[9,0]N[5,0]N[1,5]$ only. In other hand, $[10,0]E[11,0]N[7,0]S[11,0]W[10,0]W[9,0]N[5,0]N[1,5]$ and $[10,0]W[9,0]N[5,0]N[1,5]$ both are collected in every visit. Collected samples are added to $Z(s)$ to use the Monte Carlo method.  \n",
    "The Monte Carlo method is usually performed without perfect probability distribution for the environmental model, in which state-only samples cannot be used to estimate the optimal policy. Thus, it needs state-action samples. In this case, the set $Z$ is $Z(s,a)$. The results of collecting a state-action sample from every visit in the example episode are as follows:  \n",
    "\n",
    "\n",
    "$$\n",
    "\\begin{cases}\n",
    "Z(5,N) = \\{[5,0]N[1,5]\\} \\\\\n",
    "Z(7,S) = \\{[7,0]S[11,0]W[10,0]W[9,0]N[5,0]N[1,5]\\} \\\\\n",
    "Z(9,N) = \\{[9,0]N[5,0]N[1,5]\\} \\\\\n",
    "Z(10,E) = \\{[10,0]E[11,0]N[7,0]S[11,0]W[10,0]W[9,0]N[5,0]N[1,5]\\} \\\\\n",
    "Z(10,W) = \\{[10,0]W[9,0]N[5,0]N[1,5]\\} \\\\\n",
    "Z(11,N) = \\{[[11,0]N[7,0]S[11,0]W[10,0]W[9,0]N[5,0]N[1,5]\\} \\\\\n",
    "Z(11,W) = \\{[11,0]W[10,0]W[9,0]N[5,0]N[1,5]\\} \\\\\n",
    "\\end{cases}\n",
    "$$  \n",
    "Episodes can be actually collected on site or generated by simulation. Simulations use environmental models.  \n",
    "\n",
    "\n",
    "Once the training set is ready, it is very easy to evaluate the given policy $\\pi$, or to estimate the value function corresponding to $\\pi$.  \n",
    "\n",
    "\n",
    "$$\n",
    "v_\\pi(s) = {1 \\over \\left\\vert Z(s) \\right\\vert} \\sum_{z \\in Z(s)} \\mathbb{r}(z), \\forall s \\in \\mathcal{S} \\\\\n",
    "q_\\pi(s,a) = {1 \\over \\left\\vert Z(s,a) \\right\\vert} \\sum_{z \\in Z(s)} \\mathbb{r}(z), \\forall s \\in \\mathcal{S}, \\forall a \\in \\mathcal{A}\n",
    "$$  \n",
    "Monte Carlo method estimates $v_\\pi$ by the average of samples. Dynamic programming is a bootstrap method in which all states are updated together, while in the Monte Carlo method, a state can only determine its value with samples that belong to it.  \n",
    "\n",
    "\n",
    "Two problems can arise when using training sets to find optimal policies.  \n",
    "First, training sets shapes two type:$Z(s)$ and $Z(s,a)$. Using $Z(s)$ in the absence of an environmental model prevents the conversion of value functions to policies. If you have a model, you can apply $\\hat{v}(s) = \\textstyle \\underset{a \\in \\mathcal{A}(s)}{\\text{max }} \\sum_{s'} \\sum_{r} \\Pr(s', r \\vert s, a)\\left(r + \\gamma \\hat v(s')\\right), \\forall s \\in \\mathcal{S}$ to find the max behavior and convert it to a policy.  \n",
    "Second, exploration and exploitation conflict. In the off-line method of collecting a sufficiently large training set $Z$ all at once and starting the algorithm, it is not necessary to worry, but in the on-line method of generating episodes while performing the algorithm, there is a high possibility of premature convergence, which focuses on exploration and converges to the local optimal point. To avoid premature convergence, some technique must be added to the algorithm to force exploration. Usually one of two methods is used:Exploring starts, and $\\epsilon$-soft.  \n",
    "\n",
    "- Exploring starts  \n",
    "It ensures that every state-action pair occurs evenly when generating an episode. Exploring start is one where all combinations of values that a state can have and values that an action can have, have a probability of occurrence greater than zero, thus excluding combinations that are never attempted.  \n",
    "- $\\epsilon$-soft  \n",
    "It leaves the possibility of being selected by assigning a certain probability to state-action outside the mainstream.  \n",
    "$$\n",
    "\\Pr(a \\vert s) = \n",
    "\\begin{cases}\n",
    "1 - \\epsilon + {\\epsilon \\over \\left\\vert \\mathcal{A}(s)\\right\\vert}, & \\text{if } a = a' \\\\\n",
    "{\\epsilon \\over \\left\\vert \\mathcal{A}(s)\\right\\vert}, & \\text{if } a \\ne a'\n",
    "\\end{cases}\n",
    "$$\n",
    "It allocates small probabilities to the remaining actions that are not selected as optimal actions so that they can be selected even if they are not currently optimal, contributing to policy improvement.  \n",
    "\n",
    "\n",
    "**Features of the Monte Carlo Method**\n",
    "1. It doesn't need the environment model.\n",
    "2. Since it is not a bootstrap method, it is possible to estimate the optimal value and optimal policy only for a subset consisting of the states of interest only.\n",
    "3. The performance degradation is relatively small even in the case of a large deviation from the Markov property.\n",
    "\n",
    "\n",
    "## Temporal Difference learning\n",
    "TD(temporal difference) learning is an algorithm that combines the advantages of dynamic programming and the Monte Carlo method.  \n",
    "\n",
    "\n",
    "In Monte Carlo method, if sample $z_t$ is added to the $k^{th}$, value function can be written as  \n",
    "\n",
    "\n",
    "$$\n",
    "v_\\pi^{(new)}(s_t) = {v_\\pi^{(old)}(s_k) * (k - 1) \\over k} + {\\mathbb{r}(z_t) \\over k} = v_\\pi^{(old)}(s_t) + {1 \\over k}\\left(\\mathbb{r}(z_t) - v_\\pi^{(old)}(s_t)\\right)\n",
    "$$\n",
    "and it also expressed as  \n",
    "\n",
    "\n",
    "$$\n",
    "v_\\pi(s_t) = v_\\pi(s_t) + \\rho\\left(\\mathbb{r}_{new} - v_\\pi(s_t)\\right)\n",
    "$$\n",
    "In the Monte Carlo method, the update does not take place until the end state $s_T$ is reached. Unlike the Monte Carlo method, TD learning is updated immediately after entering the next state.  \n",
    "\n",
    "\n",
    "$$\n",
    "v_\\pi(s_t) = v_\\pi(s_t) + \\rho\\Bigl(\\bigl(r_{t+1} + \\gamma v_\\pi(s_{t+1})\\bigr) - v_\\pi(s_t)\\Bigr)\n",
    "$$\n",
    "This equation is defined as the next state $s_{t+1}$ and the next reward $r_{t+1}$. $\\gamma$ is a discount rate.  \n",
    "The above equation is a formula for estimating the state value function, and the following is an estimating state-action value function.  \n",
    "\n",
    "\n",
    "$$\n",
    "q_\\pi(s_t,a_t) = q_\\pi(s_t,a_t) + \\rho\\Bigl(\\bigl(r_{t+1} + \\gamma q_\\pi(s_{t+1},a_{t+1})\\bigr) - q_\\pi(s_t,a_t)\\Bigr)\n",
    "$$  \n",
    "\n",
    "\n",
    "Algorithms that find the optimal policy using TD learning ideas include Sarsa and Q-learning.  \n",
    "- **Sarsa**  \n",
    "It uses state-action algorithm.  \n",
    "$$\n",
    "q_\\pi(s_t,a_t) = q_\\pi(s_t,a_t) + \\rho\\Bigl(\\bigl(r_{t+1} + \\gamma q_\\pi(s_{t+1},a_{t+1})\\bigr) - q_\\pi(s_t,a_t)\\Bigr)\n",
    "$$\n",
    "Instead of creating an action with an explicit policy $\\pi$, $q$ takes the place of $\\pi$. So improving $q$ implicitly improves policy. If it is determined that the algorithm action is likely to be exploratory, then $\\epsilon$-soft is applied. Sarsa decides the next action based on policy, which is called on-policy.  \n",
    "- **Q-learning**  \n",
    "$$\n",
    "q_\\pi(s_t,a_t) = q_\\pi(s_t,a_t) + \\rho\\Bigl(\\bigl(r_{t+1} + \\gamma \\underset{a}{\\text{ max }} q_\\pi(s_{t+1},a)\\bigr) - q_\\pi(s_t,a_t)\\Bigr)\n",
    "$$\n",
    "In Q-learning, it is called off-policy because it uses $\\text{max}$ instead of policy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

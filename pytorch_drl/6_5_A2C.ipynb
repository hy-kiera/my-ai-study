{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "ENV = 'CartPole-v0'\n",
    "GAMMA = 0.99 # 시간할인율\n",
    "MAX_STEPS = 200 # 1에피소드당 최대 단계 수\n",
    "NUM_EPISODES = 1000 # 최대 에피소드 수\n",
    "\n",
    "NUM_PROCESSES = 32 # 동시 실행 환경 수\n",
    "NUM_ADVANCED_STEP = 5 # 총 보상을 계산할 때 Advantage 학습을 할 단계수\n",
    "# A2C 손실함수 계산에 사용되는 상수\n",
    "value_loss_coef = 0.5\n",
    "entropy_coef = 0.01\n",
    "max_grad_norm = 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Memory class\n",
    "\n",
    "class RolloutStorage(object):\n",
    "    '''Advantage 학습에 사용할 메모리 클래스'''\n",
    "    def __init__(self, num_steps, num_processes, obs_shape):\n",
    "        self.observations = torch.zeros(num_steps + 1, num_processes, 4)\n",
    "        self.masks = torch.ones(num_steps + 1, num_processes, 1)\n",
    "        self.rewards = torch.zeros(num_steps, num_processes, 1)\n",
    "        self.actions = torch.zeros(num_steps, num_processes, 1).long()\n",
    "        \n",
    "        self.returns = torch.zeros(num_steps + 1, num_processes, 1)\n",
    "        self.index = 0\n",
    "        \n",
    "    def insert(self, current_obs, action, reward, mask):\n",
    "        '''현재 인덱스 위치에 transition을 저장'''\n",
    "        self.observations[self.index + 1].copy_(current_obs)\n",
    "        self.masks[self.index + 1].copy_(mask)\n",
    "        self.rewards[self.index].copy_(reward)\n",
    "        self.actions[self.index].copy_(action)\n",
    "        \n",
    "        self.index = (self.index + 1) % NUM_ADVANCED_STEP\n",
    "        \n",
    "    def after_update(self):\n",
    "        '''Advantage 학습 단계만큰 단계가 진행되면 가장 새로운 transition을 indexd0에 저장'''\n",
    "        self.observations[0].copy_(self.observations[-1])\n",
    "        self.masks[0].copy_(self.masks[-1])\n",
    "        \n",
    "    def compute_returns(self, next_value):\n",
    "        '''Advantage 학습 범위 안의 각 단계에 대해 할인 총보상을 계산'''\n",
    "        self.returns[-1] = next_value\n",
    "        for ad_step in reversed(range(self.rewards.size(0))):\n",
    "            self.returns[ad_step] = self.returns[ad_step + 1] * GAMMA * self.masks[ad_step + 1] + self.rewards[ad_step]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Neural Net\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self, n_in, n_mid, n_out):\n",
    "        super(Net, self).__init__()\n",
    "        self.fc1 = nn.Linear(n_in, n_mid)\n",
    "        self.fc2 = nn.Linear(n_mid, n_mid)\n",
    "        self.actor = nn.Linear(n_mid, n_out) # 행동 결정\n",
    "        self.critic = nn.Linear(n_mid, 1) # 상태가치 출력\n",
    "        \n",
    "    def forward(self, x):\n",
    "        '''신경망 순전파 계산을 정의'''\n",
    "        h1 = F.relu(self.fc1(x))\n",
    "        h2 = F.relu(self.fc2(h1))\n",
    "        critic_output = self.critic(h2)\n",
    "        actor_output = self.actor(h2)\n",
    "        \n",
    "        return critic_output, actor_output\n",
    "    \n",
    "    def act(self, x):\n",
    "        value, actor_output = self(x)\n",
    "        action_probs = F.softmax(actor_output, dim=1)\n",
    "        action = action_probs.multinomial(num_samples=1)\n",
    "        \n",
    "        return action\n",
    "    \n",
    "    def get_value(self, x):\n",
    "        value, actor_output = self(x)\n",
    "        \n",
    "        return value\n",
    "    \n",
    "    def evaluate_actions(self, x, actions):\n",
    "        '''상태 x로부터 상태가치, 실제 행동 actions의 고르 확률, 엔트로피 계산'''\n",
    "        value, actor_output = self(x)\n",
    "        \n",
    "        log_probs = F.log_softmax(actor_output, dim=1)\n",
    "        action_log_probs = log_probs.gather(1, actions)\n",
    "        \n",
    "        probs = F.softmax(actor_output, dim=1)\n",
    "        entropy = -(log_probs * probs).sum(-1).mean()\n",
    "        \n",
    "        return value, action_log_probs, entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import optim\n",
    "\n",
    "class Brain(object):\n",
    "    def __init__(self, actor_critic):\n",
    "        self.actor_critic = actor_critic\n",
    "        self.optimizer = optim.Adam(self.actor_critic.parameters(), lr=0.01)\n",
    "        \n",
    "    def update(self, rollouts):\n",
    "        obs_shape = rollouts.observations.size()[2:] # torch.Size([4, 84, 84])\n",
    "        num_steps = NUM_ADVANCED_STEP\n",
    "        num_processes = NUM_PROCESSES\n",
    "        \n",
    "        values, action_log_probs, entropy = self.actor_critic.evaluate_actions(\n",
    "                                                    rollouts.observations[:-1].view(-1, 4),\n",
    "                                                    rollouts.actions.view(-1, 1))\n",
    "        # rollouts.observations[:-1].view(-1, 4) -> torch.Size([80, 4])\n",
    "        # rollouts.actions.view(-1, 1) -> torch.Size([80, 1])\n",
    "        # values -> torch.Size([80, 1])\n",
    "        # actions_log_probs -> torch.Size([80, 1])\n",
    "        # entropy -> torch.Size([])\n",
    "        \n",
    "        values = values.view(num_steps, num_processes, 1) # torch.Size([5, 16, 1])\n",
    "        action_log_probs = action_log_probs.view(num_steps, num_processes, 1)\n",
    "        \n",
    "        # advantage(행동가치 - 상태가치) 계산\n",
    "        advantages = rollouts.returns[:-1] - values # torch.Size([5, 16, 1])\n",
    "        \n",
    "        # loss of Critic\n",
    "        value_loss = advantages.pow(2).mean()\n",
    "        \n",
    "        action_gain = (action_log_probs * advantages.detach()).mean()\n",
    "        \n",
    "        total_loss = (value_loss * value_loss_coef - action_gain - entropy * entropy_coef)\n",
    "        \n",
    "        self.actor_critic.train()\n",
    "        self.optimizer.zero_grad()\n",
    "        total_loss.backward()\n",
    "        nn.utils.clip_grad_norm_(self.actor_critic.parameters(), max_grad_norm)\n",
    "        \n",
    "        self.optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "\n",
    "class Envionment:\n",
    "    def run(self):\n",
    "        '''실행 엔트리 포인트'''\n",
    "        \n",
    "        envs = [gym.make(ENV) for i in range(NUM_PROCESSES)]\n",
    "        \n",
    "        n_in = envs[0].observation_space.shape[0]\n",
    "        n_out = envs[0].action_space.n\n",
    "        n_mid = 32\n",
    "        actor_critic = Net(n_in, n_mid, n_out)\n",
    "        global_brain = Brain(actor_critic)\n",
    "        \n",
    "        obs_shape = n_in\n",
    "        current_obs = torch.zeros(NUM_PROCESSES, obs_shape) # torch.Size([16, 4])\n",
    "        rollouts = RolloutStorage(NUM_ADVANCED_STEP, NUM_PROCESSES, obs_shape)\n",
    "        episode_rewards = torch.zeros([NUM_PROCESSES, 1])\n",
    "        final_rewards = torch.zeros([NUM_PROCESSES, 1])\n",
    "        obs_np = np.zeros([NUM_PROCESSES, obs_shape])\n",
    "        reward_np = np.zeros([NUM_PROCESSES, 1])\n",
    "        done_np = np.zeros([NUM_PROCESSES, 1])\n",
    "        each_step = np.zeros(NUM_PROCESSES)\n",
    "        episode = 0\n",
    "        \n",
    "        obs = [envs[i].reset() for i in range(NUM_PROCESSES)]\n",
    "        obs = np.array(obs)\n",
    "        obs = torch.from_numpy(obs).float()\n",
    "        current_obs = obs\n",
    "        \n",
    "        rollouts.observations[0].copy_(current_obs)\n",
    "        \n",
    "        for j in range(NUM_EPISODES*NUM_PROCESSES):\n",
    "            for step in range(NUM_ADVANCED_STEP):\n",
    "                # choice an action\n",
    "                with torch.no_grad():\n",
    "                    action = actor_critic.act(rollouts.observations[step])\n",
    "                    \n",
    "                actions = action.squeeze(1).numpy()\n",
    "                \n",
    "                # 1 step\n",
    "                for i in range(NUM_PROCESSES):\n",
    "                    obs_np[i], reward_np[i], done_np[i], _ = envs[i].step(actions[i])\n",
    "                    \n",
    "                    # episode의 종료가치, state_next를 설정\n",
    "                    if done_np[i]:\n",
    "                        if i == 0:\n",
    "                            print('%d Episode : Finished after %d steps' %(episode, each_step[i]+1))\n",
    "                            episode += 1\n",
    "                            \n",
    "                        if each_step[i] < 195:\n",
    "                            reward_np[i] = -1.0\n",
    "                        else:\n",
    "                            reward_np[i] = 1.0\n",
    "                            \n",
    "                        each_step[i] = 0\n",
    "                        obs_np[i] = envs[i].reset()\n",
    "                        \n",
    "                    else:\n",
    "                        reward_np[i] = 0.0\n",
    "                        each_step[i] += 1\n",
    "                        \n",
    "                reward = torch.from_numpy(reward_np).float()\n",
    "                episode_rewards += reward\n",
    "                \n",
    "                masks = torch.FloatTensor([[0.0] if done_ else [1.0] for done_ in done_np])\n",
    "                \n",
    "                final_rewards *= masks\n",
    "                final_rewards += (1 - masks) * episode_rewards\n",
    "                \n",
    "                episode_rewards *= masks\n",
    "                \n",
    "                current_obs *= masks\n",
    "                \n",
    "                obs = torch.from_numpy(obs_np).float() # torch.Size([16, 4])\n",
    "                current_obs = obs\n",
    "                \n",
    "                # 메모리 객체에 현 단계의 transition을 저장\n",
    "                rollouts.insert(current_obs, action.data, reward, masks)\n",
    "                \n",
    "            # advanced 학습 for 문 끝\n",
    "            \n",
    "            # advanced 학습 대상 중 마지막 단계의 상태로 예측하는 상태가치를 계산\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                next_value = actor_critic.get_value(rollouts.observations[-1]).detach()\n",
    "                # rollouts.observations -> torch.Size([6, 16, 4])\n",
    "                \n",
    "            rollouts.compute_returns(next_value)\n",
    "            \n",
    "            global_brain.update(rollouts)\n",
    "            rollouts.after_update()\n",
    "            \n",
    "            if final_rewards.sum().numpy() >= NUM_PROCESSES:\n",
    "                print('success')\n",
    "                break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 Episode : Finished after 37 steps\n",
      "1 Episode : Finished after 14 steps\n",
      "2 Episode : Finished after 26 steps\n",
      "3 Episode : Finished after 19 steps\n",
      "4 Episode : Finished after 23 steps\n",
      "5 Episode : Finished after 25 steps\n",
      "6 Episode : Finished after 15 steps\n",
      "7 Episode : Finished after 19 steps\n",
      "8 Episode : Finished after 16 steps\n",
      "9 Episode : Finished after 14 steps\n",
      "10 Episode : Finished after 17 steps\n",
      "11 Episode : Finished after 200 steps\n",
      "12 Episode : Finished after 75 steps\n",
      "13 Episode : Finished after 148 steps\n",
      "14 Episode : Finished after 200 steps\n",
      "15 Episode : Finished after 200 steps\n",
      "16 Episode : Finished after 197 steps\n",
      "17 Episode : Finished after 200 steps\n",
      "18 Episode : Finished after 10 steps\n",
      "19 Episode : Finished after 70 steps\n",
      "20 Episode : Finished after 9 steps\n",
      "21 Episode : Finished after 11 steps\n",
      "22 Episode : Finished after 11 steps\n",
      "23 Episode : Finished after 14 steps\n",
      "24 Episode : Finished after 118 steps\n",
      "25 Episode : Finished after 200 steps\n",
      "26 Episode : Finished after 17 steps\n",
      "27 Episode : Finished after 200 steps\n",
      "28 Episode : Finished after 148 steps\n",
      "29 Episode : Finished after 21 steps\n",
      "30 Episode : Finished after 118 steps\n",
      "31 Episode : Finished after 137 steps\n",
      "32 Episode : Finished after 149 steps\n",
      "33 Episode : Finished after 163 steps\n",
      "34 Episode : Finished after 200 steps\n",
      "35 Episode : Finished after 200 steps\n",
      "success\n"
     ]
    }
   ],
   "source": [
    "cartpole_env = Envionment()\n",
    "cartpole_env.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
